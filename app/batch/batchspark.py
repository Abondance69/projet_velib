from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col,
    avg,
    min as spark_min,
    max as spark_max,
    sum as spark_sum
)

# ============================================================
#  âš™ï¸ Initialisation de la SparkSession + config MongoDB
# ============================================================
spark = (
    SparkSession.builder
    .appName("velib-batch")
    .master("local[*]")  # OK dans ton contexte docker
    .config(
        "spark.jars.packages",
        "org.mongodb.spark:mongo-spark-connector_2.12:10.3.0"
    )
    # URI par dÃ©faut (Ã©criture)
    .config(
        "spark.mongodb.write.connection.uri",
        "mongodb://admin:pwd@mongodb-ipssi:27017/"
        "velib.velib_batch_capacity?authSource=admin"
    )
    # URI par dÃ©faut (lecture si besoin)
    .config(
        "spark.mongodb.read.connection.uri",
        "mongodb://admin:pwd@mongodb-ipssi:27017/"
        "velib.velib_batch_capacity?authSource=admin"
    )
    .getOrCreate()
)

print("ğŸš€ Spark session started")

# ============================================================
#  ğŸ“¥ Lecture du CSV Velib depuis HDFS
# ============================================================
df = (
    spark.read
    .option("header", True)
    .option("inferSchema", True)
    .option("delimiter", ";")
    .csv("hdfs://namenode:9000/users/ipssi/input/velib.csv")
)

print("ğŸ“„ DonnÃ©es brutes :")
df.show(5)

# ============================================================
#  ğŸ§¹ Nettoyage / typage des colonnes utiles
# ============================================================
df_clean = (
    df
    .withColumn("capacity", col("capacity").cast("int"))
    .withColumn("stationcode", col("stationcode").cast("int"))
    .withColumn("numdocksavailable", col("numdocksavailable").cast("int"))
    .withColumn("numbikesavailable", col("numbikesavailable").cast("int"))
    .withColumn("mechanical", col("mechanical").cast("int"))
    .withColumn("ebike", col("ebike").cast("int"))
)

# ============================================================
#  ğŸ§® Calculs batch
# ============================================================

# ğŸ”¹ CapacitÃ© totale par station (par nom de station)
capacity_by_station = (
    df_clean
    .groupBy("name")
    .agg(spark_sum("capacity").alias("total_capacity"))
)

# ğŸ”¹ Statistiques globales sur la capacitÃ©
stats = df_clean.select(
    avg("capacity").alias("moyenne"),
    spark_min("capacity").alias("min"),
    spark_max("capacity").alias("max"),
)

print("ğŸ CapacitÃ© totale par station :")
capacity_by_station.show(20, truncate=False)

print("ğŸ“Š Statistiques globales :")
stats.show()

# ============================================================
#  ğŸ’¾ Ã‰criture dans MongoDB (ROLE 2)
# ============================================================
print("ğŸ’¾ Insertion dans MongoDB...")

# 1ï¸âƒ£ Table agrÃ©gÃ©e par station
capacity_by_station.write \
    .format("mongodb") \
    .mode("append") \
    .option("connection.uri", "mongodb://admin:pwd@mongodb-ipssi:27017") \
    .option("database", "velib") \
    .option("collection", "velib_batch_capacity") \
    .option("authSource", "admin") \
    .save()

# 2ï¸âƒ£ Table des stats globales
stats.write \
    .format("mongodb") \
    .mode("append") \
    .option("connection.uri", "mongodb://admin:pwd@mongodb-ipssi:27017") \
    .option("database", "velib") \
    .option("collection", "velib_batch_stats") \
    .option("authSource", "admin") \
    .save()

print("âœ… DonnÃ©es batch Ã©crites dans Mongo !")

spark.stop()
